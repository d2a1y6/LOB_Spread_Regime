这份备忘录详细梳理了你的数据从原始数据库提取到最终进入模型训练的全过程。鉴于你在研究微观结构（LOB & Spread），数据处理中的一些微小细节（如缩放因子、时间对齐、去平盘）对结论有重大影响。

以下是基于你提供的代码文件分析得出的**数据处理流水线备忘录**。

------

# 📝 数据处理流水线备忘录 (Data Pipeline Memo)

日期：2025-01-19

主题：LOB-Spread-Regime 项目数据处理全流程审计

适用范围：沪市 Level-2 数据 (2025.07.21 - 2025.07.25)

------

## 1. 原始数据提取与特征工程 (ETL)

**脚本来源**：`01_GetDataFromDB_sh.dos` (DolphinDB)

这一步决定了数据的“基因”。最大的特点是采用了**3秒快照聚合**，并对极小数值进行了**放大处理**。

- **数据源**：

  - **Snapshot** (`MarketData`): 盘口快照。
  - **Tick** (`mdl_4_24_0`): 逐笔成交。

- **时间网格 (Time Grid)**：

  - 严格限定在 `09:30:00 - 11:30:00` 和 `13:00:00 - 15:00:00`。
  - 重采样频率：**每 3秒 (3000ms)** 一个点。

- **聚合逻辑**：

  - Snapshot 数据使用 `aj` (As-of Join) 对齐到 3秒 时刻，取最近时刻的盘口状态。
  - Tick 数据使用 `wj` (Window Join) 聚合，统计**过去 3秒内**的成交总量 (`VolumeAll`)、最大单笔成交 (`VolumeMax`) 和 净主动买入量 (`Accum_Vol_Diff`, 即 `TradeVol_Net`)。

- ⚠️ 关键特征定义与缩放 (Scaling Strategy)：

  为了防止保存 CSV 时精度丢失，代码在输出前对部分特征进行了乘法放大。在解读 SHAP 图坐标轴时必须除回这些倍数！

  - `Relative_Spread` (相对价差): **放大 10,000 倍** (单位：BP)。
  - `Micro_Mid_Spread` (微观重心偏移): **放大 1,000 倍**。
  - `PastReturn` (对数收益率): **放大 10,000 倍**。
  - `Lambda` (Amihud非流动性): **放大 10^9 倍**。
  - `Volatility` (波动率): **放大 10,000 倍**。
  - `AutoCov` (自协方差): **放大 10^7 倍**。

- **方向定义 (LobImbalance)**：

  - 公式：`(AskVol - BidVol) / (AskVol + BidVol)`
  - **正值 (+)**：代表卖压大（卖单多）。
  - **负值 (-)**：代表买压大（买单多）。
  - *注：这与常见的 (Bid-Ask)/(Bid+Ask) 定义符号相反，需注意解读方向。*

## 2. 数据合并与物理重排

**脚本来源**：`02_ConcatDataFromDB.py`

这一步将分散的 CSV 整合为高效的 Parquet 格式，并确定了数据的物理存储顺序。

- **合并范围**：读取所有 `sh_*.csv` 和 `sz_*.csv`。
- **强制排序**：
  - 优先按 `Stock` 排序，次级按 `Time` 排序。
  - **意义**：保证了后续计算滞后特征（Shift/Rolling）时，同一只股票的数据在内存中是连续的，防止不同股票的数据混杂。
- **存储格式**：Parquet (Snappy压缩)，保留了 `Time` 列的 datetime 格式。

## 3. 预处理、清洗与标签生成 (Pre-processing)

**脚本来源**：`03_PreprocessDataForTrain.py`

这是进入模型前的最后一道关卡，包含了几个具有强主观性的**设计决策 (Design Choices)**。

### A. 标签生成 (Labeling)

- **预测目标**：未来 3秒 的中间价变动 (`Next_MidPrice - MidPrice`)。
- **⚠️ 关键过滤 (Filtering)**：
  - **剔除平盘**：代码显式删除了 `Target_Diff == 0` 的样本。
  - **影响**：模型被训练为**“在价格发生变动的前提下，预测是涨还是跌”**。它不会预测“价格不变”。这是一个很强的假设，但也去除了大量噪音。
- **二分类**：涨 = 1，跌 = 0。

### B. 强力清洗

- **去空值**：删除了任何包含 `NaN` 的行。
  - 这通常会损失掉每天开盘的前几行数据（因为 `Volatility` 等特征需要历史窗口计算，开盘瞬间窗口数据不足）。

### C. 数据集切分 (Time-Series Split)

- **切分比例**：前 80% 时间做训练，后 20% 时间做测试。
- **防泄露机制**：
  - 代码包含一段逻辑：如果 80% 切分点恰好落在某天收盘（如 15:00），会自动延后到次日开盘。
  - **目的**：防止测试集的第一条数据回看历史窗口时跨越到训练集的最后一天，导致细微的信息泄露。

### D. 特征标准化 (Normalization)

- **方法**：Z-Score (`StandardScaler`)，即减均值除以标准差。
- **严格防泄露**：
  - `scaler.fit(X_train)`：只在训练集上计算均值和方差。
  - `scaler.transform(X_test)`：用训练集的参数去转换测试集。

## 4. 最终入模特征列表 (Feature List)

最终进入 LightGBM 模型的数据包含以下 13 个特征（已标准化）：

1. `Accum_Vol_Diff` (净主动买入量)
2. `VolumeMax` (最大单笔成交)
3. `VolumeAll` (成交总量)
4. `Immediacy` (成交紧迫性/频率)
5. `Depth_Change` (深度变化)
6. `LobImbalance` (挂单不平衡，正=卖压，负=买压)
7. `DeepLobImbalance` (深层挂单不平衡)
8. `Relative_Spread` (相对价差)
9. `Micro_Mid_Spread` (微观重心偏移)
10. `PastReturn` (过去3s收益)
11. `Lambda` (Amihud非流动性指标)
12. `Volatility` (波动率)
13. `AutoCov` (自协方差)

------

## 🧐 研究中需注意的潜在风险 (Risk Assessment)

1. **3秒聚合掩盖微观结构**：A股实际上有更细粒度的 Tick 数据（深市是逐笔，沪市是3秒快照但有逐笔成交）。如果你研究的机制（Spread=2的反转）发生在毫秒级撤单中，当前的 **3秒聚合** 可能会平滑掉这些动作，导致你看到的是“结果”而非“过程”。
   - *应对*：在论文中明确指出这是基于上交所 3秒 快照机制的研究，而不是逐笔高频研究。
2. **平盘数据剔除**：训练时剔除了 `Label=Flat` 的数据。如果测试集或实际应用中包含大量平盘数据，模型的表现可能会有偏差（它倾向于强行给出一个涨跌方向，即使市场不动）。
3. **Scaling 解读**：在 SHAP 图中看到的 `Micro_Mid_Spread` 值（比如 0.4 或 -0.4），实际上对应的是被放大后的值。如果未除以 1000，**切勿直接将其解释为“偏离了 0.4 元”**（那太巨大了），它可能代表偏离了 0.0004 元。
   - 验证：`Micro_Mid_Spread` 放大倍数是 1000。图中的 0.4 对应原始值 0.0004。考虑到 A股 Tick=0.01，这个量级是合理的（重心的微小偏移）。